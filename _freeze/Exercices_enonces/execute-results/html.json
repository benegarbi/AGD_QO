{
  "hash": "74b6e624ffcd2022872028b9db19ad46",
  "result": {
    "markdown": "---\ntitle: \"Application avec R.temis dans RStudio\"\n---\n\n\n# Ressources\n\n**Pas à Pas** de l'analyse de réponses à la question ouverte K1 [Enquête \"Population, Espace de Vie, Environnement\", Ined 1992](<https://data.ined.fr/index.php/catalog/41>) avec caractéristiques des répondants : *QO_Pee_K1.html*\\\net aussi le *Pas à Pas montrant toutes les fonctionnalités de R.temis* appliqué sur un autre texte court <https://rtemis.hypotheses.org/r-temis-dans-rstudio>\n\n***Extraire les scripts nécessaires des pages citées plus haut***\n\n**Fichiers fournis**\n\n-   Extrait des données de l'enquête : *PEE_K1_extract.csv*\n\n-   Lemmatiseur construit à partir du lexique associé aux données : *Pee_dic_lem_extract.csv*\n\n-   Lemmatiseur adapté de Lexique 3 [base de données lexicales du français](https://chrplr.github.io/openlexicon/datasets-info/Lexique382/README-Lexique.html) contenant des représentations orthographiques et phonémiques, des lemmes associés ainsi que leur catégorie grammaticale ... : *Lexique383_simplifié.csv*\n\n## Etape 1 : Créer son environnement de travail\n\n1.  Créer un répertoire/dossier pour l'analyse sur votre ordinateur. Dans ce dossier, créér un sous répertoire appelé par exemple *data* pour y placer les données à utiliser.\n\n2.  Créer un projet R dans le dossier de l'analyse \\[*File/New Project* ...\\]. Créer un script \\[*File/New File* ...\\].\n\n3. Télécharger les données extraites de l'enquête PEE (textes et métadonnées) (**PEE_K1_extract.csv***)* dans le répertoire *data* en exécutant le script ci-dessous.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nPEE_K1_extract <- read.csv(\"https://raw.githubusercontent.com/benegarbi/AtelierQO_R/master/data/PEE_K1_extract.csv\", sep=\";\")\n\nwrite.csv2(PEE_K1_extract,file=\"data/PEE_K1_extract.csv\")\n```\n:::\n\n\nVisualiser les données à traiter\n\n\n## Etape 2 : Importer les données dans R\n\nImporter ce fichier contenant une variable textuelle et des métadonnées avec le *package R.temis*.\n\n1.  Importer le fichier `import_corpus`.\n\n2.  Créer le tableau lexical (avec mots-outils) `build_dtm`.\n\n3.  Repérer le nombre de répondants et le nombre de mots différents cités dans les réponses.\n\n4.  Créer le dictionnaire `dictionary` puis le visualiser `View`. Le trier, repérer les mots les plus cités, les lemmes générés par R.\n\n5.  Afficher des concordances avec des mots (au choix) `concordances`.\n\n6.  Afficher le nuage de mots associés au lexique `word_cloud`. Faire ce nuage de mots avec et sans les mots-outils. Calculer les éléments qui permettront d'ajouter une légende `frequent_terms`.\n\n## Etape 3 : Repérer des cooccurrences\n\n1.  Chercher des cooccurrences à certains mots (au choix) `cooc_terms`.\n\n2.  Produire un graphe de mots pour détecter les cooccurrences autour des mots les plus fréquents `terms_graph`. Que remarquez-vous ?\n\n3.  Faire une analyse factorielle sur le tableau lexical `corpus_ca` puis explorer les résulats avec l'interface `explor`. Afficher des concordances `concordances`.\n\n## Etape 4 : Utiliser les métadonnées\n\n1.  Visualiser le tableau des métadonnnées `View(meta(corpus))`. Compter le nombre de répondants(*package questionr*) selon 3 métadonnées (au choix).\n\n2.  Faire le bilan lexical pour au moins une métadonnée (au choix)`lexical_summary`.\n\n3.  Repérer le vocabulaire spécifique pour quelques métadonnées (au choix)`specific_terms`.\n\n4.  Faire une analyse factorielle sur le tableau croisant les mots du lexique et des caractéristiques des répondants (au choix) `corpus_ca`. Explorer les résulats `explor`. Afficher des concordances `concordances`, des réponses spécifiques `extreme_docs`.\n\n## Etape 5 : Lemmatiser\n\nIl s'agit ici d'affiner peu à peu l'analyse en supprimant des mots et/ou en modifiant la lemmatisation.\n\n### Avec un lemmatiseur personnalisé\n\n1.  Créer une la liste de mots à enlever du corpus (prendre ici : \"sur\",\"que\",\"qu\"). La supprimer du tableau lexical et du lexique, puis exporter le dictionnaire `dictionary`.\n\n2.  Ouvrir ce dictionnaire avec un tableur. Corrigez si besoin les formes lemmatisées automatiquement par R.\n\n3.  Récupérer le lemmatiseur \"associé à Pee\" (**Pee_dic_lem_extract.csv)** et le copier dans le répertoire data du dossier créé pour l'analyse.\net l'importer dans le projet (toujours le répertoire data) `read.csv2`.\nPour cela, exécuter le scipt ci-dessous.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\ndic_lem1 <- read.csv2(\"https://raw.githubusercontent.com/benegarbi/AtelierQO_R/master/data/Pee_dic_lem_extract.csv\", sep=\";\",row.names=1)\n```\n:::\n\n\n\n5.  Lemmatiser le tableau lexical à l'aide de ce lemmatiseur `combine_terms`.\n6.  Compter le nombre de mots distincts restants après cette lemmatisation par exemple avec `lexical_summary`\n7.  Excécuter des analyses précédentes avec ce nouveau lexique\n\n### Avec Lexique3\n\n1.  Récupérer le lemmatiseur adapté de lexique3 (**Lexique383_simplifie.csv**) dans le répertoire data de l'analyse en exécutant le script.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlexique3 <- read.csv2(\"https://raw.githubusercontent.com/benegarbi/AtelierQO_R/master/data/Lexique383_simplifie.csv\", sep=\",\", fileEncoding=\"UTF-8\")\n```\n:::\n\n\n\n2.  Garder les mots de catégories grammaticales \"utiles\" et lemmatiser à nouveau le dictionnaire issu du tableau lexical initial\n3.  Relancer quelques analyses au choix....\n\n## Etape 6 : Classer les réponses\n\n-   Faire une classification sur le tableau lexical associé aux réponses avec `corpus_ca`. Décrire les classe `specific_terms` puis joindre exporter la variable de classe aux metadonnées..\n\n## Etape 7 : Aller plus loin\n\n-   Travailler sur des sous-corpus\n-   Utiliser le *package Rainette* pour faire la classification sur le tableau lexical <https://juba.github.io/rainette/articles/introduction_usage.html> .............\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}