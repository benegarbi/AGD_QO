---
title: "Pas à pas pour analyser une question ouverte avec R.temis"
---

## Appel du package R.temis

```{r packages,warning=FALSE,results='hide',message=FALSE}
library(R.temis)
```

## Données

Le *corpus* utilisé dans cet exemple contient un extrait des réponses à une question ouverte issue de l'enquête Populations, Espaces de Vie, Environnements (Ined, 1992).  L'intitulé de la question est celui-ci :*Si je vous dis Environnement, qu'est ce que cela évoque pour vous ?*.

Pour chaque enquêté, on dispose de caractéristiques socio-démographiques (<https://data.ined.fr/index.php/catalog/41>).

Les données sont stockées dans un tableau, dit *individus x variables*, contenant 2017 lignes (le nombre de répondants) et 16 colonnes. Ces colonnes correspondent aux variables suivantes : identifiant du questionnaire, caractéristiques des répondants (ou *métadonnées*) et en 16ème colonne la variable correspondant à la question ouverte ( *variable textuelle*). Les textes des réponses constituent le *corpus*.

```{r import}
corpus <- import_corpus("data/PEE_K1_extract.csv", format="csv",textcolumn=16,language="fr")
```

# Création du tableau lexical

La fonction `build_dtm` crée le tableau lexical (*Document Term Matrix* dans R).

```{r tle}
dtm <-build_dtm(corpus, remove_stopwords=F,min_length=2)
dtm
```

Les lignes du *tableau lexical* correspondent ici aux 2017 réponses extraites de l'enquête "Populations, Espaces de Vie, Environnements (PEE), Ined 1992) et les colonnes aux 1157 mots distincts du corpus.

## Affichage

```{r voir, eval=FALSE}
# des metadonnées
View(meta(corpus))

# des réponses à la QO
View(sapply(corpus,as.character))

# d'un extrait du Tableau lexical
inspect(dtm)
#as.matrix(dtm[1:10, c("de", "abus")])
```

# Occurrences

## Explorer le lexique

La fonction `dictionary` crée le dictionnaire associé au lexique. On peut explorer via RStudio : affichage des mots par ordre alphabétique ou par fréquence.

On affiche la liste des mots les plus fréquents avec la fonction `frequent_terms`.

```{r freqT}
dic<-dictionary(dtm,remove_stopwords = F)

frequent_terms(dtm, n=20)
```

Le mot *nature* est évoqué 888 fois dans cet extrait des réponses à la question ouverte (PEE) et représente plus de 7% des occurrences.

# Afficher le nuage de mots !

Ici affiche les mots de plus de 20 occurrences et on choisit de ne pas afficher les mots-outils (fonction `word_cloud`),

```{r nuage}
cloud<-word_cloud(dtm, color="black", n=100, min.freq=20,remove_stopwords=T)
```

Les *occurrences* de quelques mots doivent être précisées dans la légende de ce graphique pour aider sa compréhension.

# Concordances

Les *concordances* permettent, pour chaque mot, de rassembler l'ensemble des textes (ici les réponses à la question ouverte) dans lesquels il apparaît (fonction `concordances`).

Ici par exemple on cherche comprendre pourquoi le mot *enquêté* apparait dans les réponses.

```{r concordances}
concordances(corpus,dtm,"enquêté")
#concordances(corpus,dtm,"alentour")
#concordances(corpus,dtm,"sur")
#concordances(corpus,dtm,"que")
```

On doit *afficher les concordances à chaque étape* d'interprétation des résultats.

# Cooccurrences

## Terme le plus associé (positivement ou négativement) à un terme

On utilise la fonction `cooc_terms` qui affiche les termes coocurrents à un mots choisi (ici logement) dans l'ensemble du corpus.

```{r ccoc_t}
cooc_terms(dtm,"logement", n=10)
```

Parmi les réponses contenant *logement*, *mon* représente 3,5% de l'ensemble des occurrences. Plus de 10% des occurrences de *mon* sont présentes quand *logement* est aussi donné dans les réponses.

On verra par la suite qu'on peut produire un *graphe de mots*.

## Analyse factorielle sur un tableau lexical entier

L'analyse factorielle sur un tableau lexical met en évidence les mots les plus *cooccurrents*. La lecture des mots les plus contributifs aux axes et les concordances permettent d'identifier des champs lexicaux. Par la suite, elle peut aider à de justifier (ou non) la lemmatisation de certains mots (partie suivante).

```{r AFC_TLE}
resTLE <-corpus_ca(corpus,dtm, sparsity=0.985)
#explor(resTLE)
res <- explor::prepare_results(resTLE)
explor::CA_var_plot(res, xax = 1, yax = 2, lev_sup = TRUE, var_sup = FALSE,
    var_sup_choice = , var_hide = "Row", var_lab_min_contrib = 0, col_var = "Position",
    symbol_var = "Type", size_var = "Contrib", size_range = c(23.4375, 312.5),
    labels_size = 10, point_size = 25, transitions = TRUE, labels_positions = NULL,
    xlim = c(-3, 2.91), ylim = c(-2.42, 3.49))
```

Les aides à l'interprétation *classiques* (valeurs propres, contributions, coordonnées, ...) sont stockées dans *resTLE* que l'on explore avec grâce à la fonction `explor` (RShiny).

# Mettre les QO en regard avec les métadonnées

On dispose des variables suivantes : sexe, age en classes (aget), situation matrimoniale(matrim), pratique religieuse (pratique), avoir des enfants oui/non(enf), activite, profession regroupée (prof), diplôme (dipl), *vote écolo* oui/non (vote), type d'habitat, type de localité (local), revenu, region.

## Répartitions

Vérifie la répartition des caractéristiques des enquêté pour chaque métadonnée.

```{r repart, eval=FALSE}
library (questionr)

table(meta(corpus)$sexe)
table(meta(corpus)$aget)
table(meta(corpus)$matrim)
table(meta(corpus)$pratique)
table(meta(corpus)$enf)
table(meta(corpus)$activite)
table(meta(corpus)$prof)
table(meta(corpus)$dipl)
table(meta(corpus)$vote)
table(meta(corpus)$habitat)
table(meta(corpus)$localite)
table(meta(corpus)$revenu)
table(meta(corpus)$region)
```

## Nombre de mots par sous-corpus

Ici on compte les mots de chaque sous-corpus crée à partir de la variable *Region* de l'enquête.

```{r BilanLex}
lexical_summary(dtm, corpus,"region", unit = "global")           
```

Le corpus contient 12222 mots dont 1157 mots distincts, alors que le corpus des enquêtés de la région Seine contient 2242 mots dont 472 mots distints. On pourrait comparer ici le % de mots distincts (indicateur de richesse du vocabulaire) 9,5% vs 21% en région Seine.

## Specificités

### Mots spécifiques par modalités

```{r Mspec}
# selon le région
specific_terms(dtm,meta(corpus)$aget, n=10)
```

Les mots *pollution* et *nature* sont plus employés par les répondants les plus jeunes (moins de 30ans)...

### Réponses spécifiques par modalités

La fonction `characteristic_docs` affiche les documents représentatifs d'une catégorie donnée de répondants ici.

Remarque : Le calcul de la distance est fondé sur la métrique du Khi2 et mesure l'écart entre le profil d'un document et le profil moyen du document de la catégorie. Ce critère a tendance à favoriser les textes longs car plus ils contiennent de "mots", plus ils ont de chance de contenir de "mots" communs, et donc de se rapprocher du profil moyen.

```{r Rspec}
#Réponses spécifiques selon la région
characteristic_docs(corpus,dtm,meta(corpus)$region, ndocs=5)
```

# Analyse factorielle sur tableau de contingence

La fonction `corpus_ca` permet d'effectuer une AFC qui met en relation les "mots" et un ensemble de *variables qualitatives* (les métadonnées).

On choisit les variables (ici : sexe, aget, matrim, pratique, enf, activite, prof, dipl, vote, habitat, localite, revenu) pour créer le tableau de contingence croisant les *mots* du corpus et les modalités des métadonnées sur lequel on calcule l'AFC.

## Choix des metadonnées

```{r AFC_TLA}
resTLA<-corpus_ca(corpus,dtm, variables =c("sexe","aget",	"matrim","pratique","enf","activite","prof","dipl","vote",	"habitat"	,"localite","revenu"),sparsity=0.98)
#explor(resTLA)
res <- explor::prepare_results(resTLA)
explor::CA_var_plot(res, xax = 1, yax = 2, lev_sup = FALSE, var_sup = FALSE,
    var_sup_choice = , var_hide = "None", var_lab_min_contrib = 2, col_var = "Position",
    symbol_var = "Type", size_var = "Contrib", size_range = c(26.25, 350), labels_size = 10,
    point_size = 28, transitions = TRUE, labels_positions = NULL, xlim = c(-0.393,
        0.447), ylim = c(-0.413, 0.426))
```

On affiche les réponses les plus contributives aux axes 1 à 3 (fonction `extreme_docs`).

```{r extr_doc}
#Documents les plus illustratifs par axe
extreme_docs(corpus,resTLA,axis=1,ndocs=3)
extreme_docs(corpus,resTLA,axis=2,ndocs=3)
extreme_docs(corpus,resTLA,axis=3,ndocs=3)
```

# Lemmatiser

La lemmatisation (Lebart, Salem) consite à regrouper les mots qui ont une signification proche. Cette opération permet de réduire la liste de mots distincts et donc la taille du lexique.

Nous allons voir dans cette partie que cette opération peut se faire de différentes façons et nécessite de faire des choix ...

## De façon personnalisée

On voit ici comment créer son *lemmatiseur*.

### Suppression de mots

On crée une liste de mots que l'on a choisi de supprimer du tableau lexical et du dictinnaire.

Ici, comme à tout moment de l'analyse, il est important d'afficher les *concordances* d'un mot pour ensuite faire un choix (le supprimer, le regrouper avec un autre, ...).

Pour l'exemple, on supprime les mots *sur*, *que* et *qu*.
```{r nettoyage}
#concordances(corpus,dtm,"sur")
# liste des mots à supprimer
asupp <- c("sur","que","qu") 
# on enlève les mots de la liste dans le tableau lexical
dtm2 <-dtm[, !colnames(dtm) %in% asupp]
```

On crée le nouveau *dictionnaire* associé au lexique avec la fonction `dictionnary`sans les mots-outils et sans les mots que l'on a choisit de ne pas garder pour l'analyse.

```{r dico}
dic<-dictionary(dtm,remove_stopwords = T)
# on enlève les mots de la liste dans le dictionnaire
dic2 <- dic[!rownames(dic) %in% asupp,]
```

Pour créér son propre lemmatiseur, on exporte ce dictionnaire (ici dic2.csv) afin de le modifier si besoin, mots après mot (par exemple avec un tableur).

```{r Export_dic, eval=FALSE}
write.csv2(dic2, file="dic2_lem.csv")
```

### Correction de la lemmatisation de R

On pourrait utiliser la colonne Terms du dictionnaire pour lemmatiser mais celle-ci ne nous satisfait pas. On décide alors de remplacer les racines de certians mots issues de la lemmatisation automatique et on enregiste ce nouveau dictionnaire qui sera notre *lemmatiseur personnalisé*.

Ici encore, on s'aide des concordances. Par exemple : `concordances(corpus,dtm,c("écologie","écolo","écologique","écologiques","écologiste","écologistes","écolos"))`

Pour cette analyse, on utilise un lemmatiseur déjà produit lors d'une analyse précédente (avec Spad). On importe ce lemmatiseur dans R et on utilise la fonction `combine_terms` pour lemmatiser.

```{r import_lem1}
dic_lem1 <- read.csv2("data/Pee_dic_lem_extract.csv",row.names=1)

#setdiff(rownames(dic2), rownames(dic_lem1))

dtmlem1 <-combine_terms(dtm2, dic_lem1)
```

## Lexique après lemmatisation

```{r freq_lem1}
frequent_terms(dtmlem1, n=30)
```

## Utilisation de lexique3

### Pour selectionner les mots à analyser

Ici on reprend le tableau lexical et le dictionnaire initiaux (dtm et dic) càd sans suppression de mots au choix ni de lemmatisation personnalisée.

On utilise le lemmatiseur crée à partir de lexique 3 (https://chrplr.github.io/openlexicon/datasets-info/Lexique382/README-Lexique.html) qui nous permet dans un premier temps de repérer la catégorie grammaticale de chaque mot du corpus. On peut ensuite ne garder que les mots des catégories qui nous intéressent pour l'analyse (ici les adverbes *ADV*, les verbes *VER*, les adjectifs *ADJ*, les noms *NOM*, et les pronoms personnels et possessifs *PRO:per*,*PRO:pos*). On gardera pour l'analyse également les mots qui n'ont pas été identifiés grâce à Lexique3 ( mis dans la liste *nr*).

```{r gram}
library(dplyr)
lexique3 <- read.csv("data/Lexique383_simplifie.csv", fileEncoding="UTF-8")
lexique3 <- arrange(lexique3, desc(freqlivres))
lexique3 <- lexique3[!duplicated(lexique3$ortho),]
voc_actif <- lexique3[lexique3$cgram %in% c("ADV", "VER", "ADJ", "NOM", "PRO:per","PRO:pos"),]
dic_total <- merge(dic, voc_actif, by.x="row.names", by.y="ortho", all.x=TRUE)
dic_total <- mutate(dic_total, Term=coalesce(lemme, Term))
rownames(dic_total) <- dic_total$Row.names

# Lister les mots non reconnus par Lexique 3  
nr <- filter(dic_total, is.na(lemme))

```

## Lemmatisation automatique avec lexique3

Cette opération permet à la fois de ne retenir que certaines catégories de mots mais aussi à les remplacer par leur forme racine.

```{r lem_lex3}
dtmlem2 <- combine_terms(dtm, dic_total)
```

## Lexique après lemmatisation avec Lexique3

```{r freq_lex3}
frequent_terms(dtmlem2, n=30)
```

## Graphe de mots

```{r tree,, eval=FALSE}
arbre <- terms_graph(dtmlem2, min_occ = 30, interactive=F)
```

![](images/graphe_mots.png)

# Nouvelles AFC

## Suite à la lemmatisation avec lexique3

```{r AFC_TLElem2}
resTLElem2 <-corpus_ca(corpus,dtmlem2,sparsity=0.985 )
#explor(resTLElem2)

res <- explor::prepare_results(resTLElem2)
explor::CA_var_plot(res, xax = 1, yax = 2, lev_sup = FALSE, var_sup = FALSE,
    var_sup_choice = , var_hide = "Row", var_lab_min_contrib = 0, col_var = "Position",
    symbol_var = "Type", size_var = "Contrib", size_range = c(23.4375, 312.5),
    labels_size = 10, point_size = 25, transitions = TRUE, labels_positions = NULL,
    xlim = c(-2.55, 9.92), ylim = c(-4.66, 7.81))
```

```{r AFC_TLAlem2}
resTLAlem2<-corpus_ca(corpus,dtmlem2, variables =c("sexe","aget",	"matrim","pratique","enf","activite","prof","dipl","vote",	"habitat"	,"localite","revenu"),sparsity=0.98)
#explor(resTLAlem2)
res <- explor::prepare_results(resTLAlem2)
explor::CA_var_plot(res, xax = 1, yax = 2, lev_sup = FALSE, var_sup = FALSE,
    var_sup_choice = , var_hide = "None", var_lab_min_contrib = 0, col_var = "Position",
    symbol_var = "Type", size_var = "Contrib", size_range = c(22.5, 300), labels_size = 10,
    point_size = 24, transitions = TRUE, labels_positions = NULL, xlim = c(-0.544,
        0.517), ylim = c(-0.482, 0.579))
```

# Et pour aller plus loin

## Correction de la lemmatisation de Lexique3

On adapte le lemmatiseur (`dic_total`) en créant un autre dictionnaire (`dicor`) avec une colonne contenant les mots qui n'ont pas été lemmatisés automatiquement. Puis on lemmatisera le tableau lexical avec les "mots" (Term) de cette nouvelle colonne. On trouve ces mots dans la liste des mots non reconnus (nr)

```{r corr_lemm2}
dic_total_c <- dic_total

dic_total_c$Term[dic_total_c$Term == "bretagn"] <- "bretagne"
dic_total_c$Term[dic_total_c$Term == "tranquil"] <- "tranquilité"

dtmlem22 <- combine_terms(dtm, dic_total_c)

```

## Analyser des sous-corpus

On créer des sous corpus à l'aide des métadonnées ou de mots au choix.

```{r sscorpus, eval=FALSE, include=FALSE}

#Ici par exemple par les réponses des enquêtes dans la région Bretagne
corpus_bret <- corpus[meta(corpus,"region") == "Bretagne"] 
# Ici les questions ouvertes contenant le mot entourage
corpus_entourage <- subset_corpus(corpus,dtm,"entourage")
```

## Classifications du tableau lexical

Va regrouper les réponses contenant des mots cooccurrents. Permet de repérer des champs lexicaux.

### avec R.temis

```{r clus}
clusTLE <- corpus_ca(corpus, dtmlem2,variables = NULL, ncp =  6, sparsity = 0.98)

#Afficher le dendrogramme
#plot(clusTLE$call$tree)
#clusTLE$desc.var
```

Ici on selectionne 7 classes

```{r cah_cut}
clus <- corpus_clustering(clusTLE,7)
```

## Description des classes

Pour chaque numéro de classe déterminé dans l'étape précédente, R affiche les mots et textes spécifiques qui vont permettre de déterminer les champs lexicaux de chacune.

On ajoute la variable de classe (clus) aux métadonnées initiales.

```{r add_clus}
corpus_cl <- add_clusters(corpus,clus)

#View(meta(corpus_cl))

#clusTLE$desc.var

```

### Mots spécifiques

On se sert de cette variable issue de la classification pour calculer les spécificités

```{r cah_desc }
specific_terms(dtmlem2,meta(corpus_cl)$clus, n=5)
```

### Réponses specifiques

specific_terms(dtmlem2,meta(corpus_cl)\$clus, n=5)

```{r doc_char2}
characteristic_docs(corpus_cl,dtmlem2,meta(corpus_cl)$clu, ndocs=3)
```

### avec Rainette (type Alceste)

On pourrait également faire une classification des questions ouvertes avec le package *Rainette*. En savoir plus sur <https://juba.github.io/rainette/articles/introduction_usage.html>

Son utilisation nesessite aussi d'appeler le package `quanteda`.

Attention risque de conflit entre les packages tm (chargé par R.temis) et quanteda : il ne faut pas lancer `library(quanteda)`.

Ici on utilise le crpus non lemmatisé

```{r reinert, eval=FALSE,message=FALSE}
library(rainette)
corpus

# On adapte le TLE pour quanteda
dfm <- quanteda::as.dfm(dtm)
quanteda::docvars(dfm, "doc_id") <- 1:nrow(dfm)

# On exécute rainette

resrai <- rainette(dfm, min_uc_size = 3, k = 10, doc_id = "doc_id", cc_test = 0.3,
  tsj = 3)
```

Exploration interactive

```{r rai_explo, eval=FALSE,message=FALSE}
rainette_explor(resrai,dfm)

groups <-cutree_rainette(resrai, k = 10)
```

![](images/dendro_rainette.png)

Description des classes

```{r rai_stat, eval=FALSE,message=FALSE}
rainette_stats(groups, dfm, n_terms = 5)
```

On récupère la variable issue de la classification Rainette pour des traitements ultérieurs

```{r rain_v_cl , eval=FALSE,message=FALSE}
meta(corpus, "classe") <- cutree_rainette(resrai, k = 10)
#View(meta(corpus))
```

